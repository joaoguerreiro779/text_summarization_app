BART is a transformer encoder-encoder (seq2seq) model with a bidirectional encoder and an autoregressive decoder. BART is pre-trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text.